{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a9eee4",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f50a3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d51a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set url and download paths\n",
    "file_id = '1ZEyNMEO43u3qhJAwJeBZxFBEYc_pVYZQ'\n",
    "zip_file = 'dataset.zip'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "extract_dir = 'extracted_contents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract dataset\n",
    "def download_and_extract_data(url, zip_file, extract_dir):\n",
    "    gdown.download(url, zip_file, quiet=False)\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract dataset\n",
    "download_and_extract_data(url=url, zip_file=zip_file, extract_dir=extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bd4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "BASE_DIR = os.path.join(extract_dir, 'dataset')\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VAL_DIR = os.path.join(BASE_DIR, 'val')\n",
    "CLASSES = ['no', 'sphere', 'vort']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data(directory, classes):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.npy'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                img = np.load(file_path)\n",
    "                # Reshape from (1, 150, 150) to (150, 150, 1) for CNN input\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "                images.append(img)\n",
    "                labels.append(idx)\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "X_train, y_train = load_data(TRAIN_DIR, CLASSES)\n",
    "X_val, y_val = load_data(VAL_DIR, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc62f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(CLASSES))\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes=len(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_val = X_val / np.max(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization function\n",
    "def visualize_samples(images, labels, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, len(images))\n",
    "        axes[i].imshow(images[idx].squeeze(), cmap='viridis')\n",
    "        class_idx = np.argmax(labels[idx]) if len(labels.shape) > 1 else labels[idx]\n",
    "        axes[i].set_title(f\"Class: {CLASSES[class_idx]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c7591",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture - using a CNN approach suitable for astronomical images\n",
    "def create_model(input_shape=(150, 150, 1)):\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fourth convolutional block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(len(CLASSES), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981c15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5aa020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define callbacks for training\n",
    "callbacks = [\n",
    "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bb4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation to improve model generalization\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Define a custom training procedure with data augmentation\n",
    "def train_with_augmentation(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=50):\n",
    "    # Create datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Apply augmentation only to training data\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x, y: (data_augmentation(x, training=True), y)\n",
    "    )\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9952022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternative approach: use the standard fit method\n",
    "def train_standard(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=50):\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6764bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose one training approach\n",
    "history = train_with_augmentation(model, X_train, y_train, X_val, y_val)\n",
    "# history = train_standard(model, X_train, y_train, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c91f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_classes, y_pred_classes, target_names=CLASSES))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASSES, yticklabels=CLASSES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3257329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try different model architectures\n",
    "\n",
    "# Transfer learning approach with ResNet50\n",
    "def create_transfer_model(input_shape=(150, 150, 1)):\n",
    "    # Convert grayscale to RGB by repeating channels\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Concatenate()([inputs, inputs, inputs])  # Duplicate the channel 3 times\n",
    "    \n",
    "    # Use pre-trained ResNet50 (without top layers)\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(150, 150, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom classification head\n",
    "    x = base_model(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(len(CLASSES), activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1ce84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
